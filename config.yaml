transformer:
  hidden_dim: 512
  num_heads: 8
  intermediate_dim: 2048
  eps: 1e-06
  num_layers: 6
  dropout: 0.1
  label_smoothing: 0.1

train:
  warmup_steps: 4000
  max_length: 128
  device: cuda
  train_batch: 96
  eval_batch: 24
  steps: 500
  epochs: 0
  val_steps: 4
  log_steps: 4
  save_steps: 4
  use_bitsandbytes: True
  scheduler_type: "warmup-consine"
  lr: 2e-3
  save_path: weights/
  dataset_name: "wmt/wmt14"

inference:
  checkpoint: PATH

tokenizer:
  tokenizer_path: PATH
  vocabulary_size: 37000
  special_tokens: ["<unk>", "<bos>", "<eos>", "<pad>"]
  unk_token: <unk>
  save_path: data

overfit_one_batch:
  tolerance: 1e-5
  max_steps: 250
  overfit: True
  hp_search: True
  min_warmup: 200
  max_warmup: 500