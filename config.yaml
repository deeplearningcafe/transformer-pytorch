transformer:
  hidden_dim: 512
  num_heads: 8
  intermediate_dim: 2048
  eps: 1e-06
  num_layers: 6
  dropout: 0.1
  label_smoothing: 0.1

train:
  warmup_steps: 4000
  max_length: 128
  device: cpu
  train_batch: 4
  eval_batch: 4
  steps: 10
  val_steps: 4
  log_steps: 4
  save_steps: 4
  use_bitsandbytes: False
  save_path: weights/
  dataset_name: "bentrevett/multi30k"

inference:
  checkpoint: CHECKPOINT-PATH

tokenizer:
  tokenizer_path: data\tokenizer.json
  vocabulary_size: 37000
  special_tokens: ["<unk>", "<bos>", "<eos>", "<pad>"]
  unk_token: <unk>
  save_path: data