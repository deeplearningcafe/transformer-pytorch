transformer:
  hidden_dim: 512
  num_heads: 8
  intermediate_dim: 2048
  eps: 1e-06
  num_layers: 6
  dropout: 0.1
  label_smoothing: 0.1
  vocabulary_size: 37000

train:
  warmup_steps: 4000
  max_length: 128
  tokenizer_path: C:\Users\victo\Documents\DataScience\python\papers_implementation\transformer-pytorch\data\tokenizer.json
  device: cpu
  train_batch: 4
  eval_batch: 4
  steps: 10
  val_steps: 1
  log_steps: 1
  save_steps: 1
  use_bitsandbytes: False
  save_path: weights/transformer_

inference:
  checkpoint: