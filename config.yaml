transformer:
  hidden_dim: 512
  num_heads: 8
  intermediate_dim: 2048
  eps: 1e-06
  num_layers: 6
  dropout: 0.1
  label_smoothing: 0.1

train:
  warmup_steps: 4000
  max_length: 128
  device: cpu
  train_batch: 16
  eval_batch: 8
  steps: 10
  epochs: 0
  val_steps: 4
  log_steps: 4
  save_steps: 4
  use_bitsandbytes: False
  save_path: weights/
  dataset_name: "bentrevett/multi30k"

inference:
  checkpoint: CHECKPOINT-PATH

tokenizer:
  tokenizer_path: TOKENIZER-PATH
  vocabulary_size: 37000
  special_tokens: ["<unk>", "<bos>", "<eos>", "<pad>"]
  unk_token: <unk>
  save_path: data

overfit_one_batch:
  tolerance: 1e-4
  max_steps: 15
  overfit: True
  hp_search: False
  min_warmup: 200
  max_warmup: 500